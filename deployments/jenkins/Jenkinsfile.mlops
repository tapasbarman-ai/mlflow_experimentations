pipeline {
    agent any

    environment {
        SCANNER_HOME = tool 'SonarScanner'
        APP_NAME = "bike-demand-predictor"
        
        // --- INFRASTRUCTURE ---
        NEXUS_REGISTRY = "localhost:8082"
        SONAR_HOST     = "sonarqube"
        MLFLOW_URI     = "http://mlflow-server:5000" // Example if using a remote server
        
        // --- TAGGING ---
        DOCKER_IMAGE = "${NEXUS_REGISTRY}/${APP_NAME}"
        NEXUS_CREDENTIALS_ID = 'nexus-creds'
        REPO_URL = "https://github.com/tapasbarman-ai/mlflow_experimentations.git"
    }

    stages {
        stage('1. Environment Setup') {
            steps {
                deleteDir()
                git branch: 'main', url: "${REPO_URL}"
                sh 'git config --global --add safe.directory "*"'
                // Ensure DVC is initialized and tracked data is pulled
                sh 'pip install --break-system-packages dvc[all] mlflow evidently fastapi uvicorn'
            }
        }

        stage('2. Data & Model Versioning (DVC)') {
            steps {
                script {
                    echo "Checking Dataset Integrity..."
                    // Requirement: Dataset version tracking mandatory
                    sh 'dvc status'
                    sh 'dvc pull || echo "No remote data found, using local cache"'
                }
            }
        }

        stage('3. Smart Training (Conditional)') {
            when {
                anyOf {
                    changeset "src/pipeline.py"
                    changeset "data/**"
                    changeset "requirements.txt"
                    expression { return params.FORCE_TRAIN == true }
                }
            }
            steps {
                script {
                    echo "Changes detected! Running ML Training Pipeline..."
                    // Requirement: Model version tagging & Reproducibility logs maintained
                    sh 'python3 src/pipeline.py'
                }
            }
        }

        stage('4. Security & Quality Scan') {
            parallel {
                stage('SAST & Secrets') {
                    steps {
                        sh 'docker run --rm --volumes-from jenkins -w $(pwd) zricethezav/gitleaks:latest detect --source . --verbose --redact || true'
                        sh 'docker run --rm --volumes-from jenkins -w $(pwd) returntocorp/semgrep semgrep scan --config auto --output semgrep-report.json || true'
                    }
                }
                stage('SonarQube Static Analysis') {
                    steps {
                        withCredentials([string(credentialsId: 'sonarqube-token', variable: 'SONAR_TOKEN')]) {
                            sh "${SCANNER_HOME}/bin/sonar-scanner \
                                -Dsonar.projectKey=bike-sharing-mlops \
                                -Dsonar.sources=src/ \
                                -Dsonar.host.url=http://${SONAR_HOST}:9000 \
                                -Dsonar.login=\$SONAR_TOKEN"
                        }
                    }
                }
            }
        }

        stage('5. Bias & Drift Evaluation') {
            steps {
                script {
                    // Requirement: Bias evaluation report before release & Model drift monitoring
                    echo "Evaluating Model for Fairness and Drift..."
                    // These reports are generated by pipeline.py and stored in artifacts/
                    archiveArtifacts artifacts: 'artifacts/*/bike_drift_report.html', allowEmptyArchive: false
                    archiveArtifacts artifacts: 'artifacts/*/bike_cm.png', allowEmptyArchive: false
                }
            }
        }

        stage('6. Build & Containerize Serve API') {
            steps {
                script {
                    echo "Building Production Container..."
                    // Note: This container runs serve.py, pulling the production model from MLflow at runtime
                    sh "docker build -t ${DOCKER_IMAGE}:${BUILD_NUMBER} -f deployments/docker/Dockerfile ."
                    sh "docker tag ${DOCKER_IMAGE}:${BUILD_NUMBER} ${DOCKER_IMAGE}:latest"
                }
            }
        }

        stage('7. Compliance & Audit Logging Test') {
            steps {
                script {
                    // Requirement: Inference logging where contractually required
                    echo "Verifying Inference Logging and Audit Contracts..."
                    // Start container temporarily to run integration tests
                    sh "docker run -d --name test-api -p 8000:8000 ${DOCKER_IMAGE}:${BUILD_NUMBER}"
                    sleep 5
                    sh 'curl -X POST http://localhost:8000/predict -d \'{"hr":10, "holiday":0, "workingday":1}\' -H "Content-Type: application/json"'
                    // Check if logs are generated (verify your serve.py logging logic)
                    sh "docker logs test-api | grep 'Prediction success'"
                    sh "docker stop test-api && docker rm test-api"
                }
            }
        }

        stage('8. Image Security & Push') {
            steps {
                sh "docker run --rm -v /var/run/docker.sock:/var/run/docker.sock aquasec/trivy image --severity HIGH,CRITICAL ${DOCKER_IMAGE}:${BUILD_NUMBER}"
                withCredentials([usernamePassword(credentialsId: "${NEXUS_CREDENTIALS_ID}", usernameVariable: 'USER', passwordVariable: 'PASS')]) {
                    sh "echo \$PASS | docker login -u \$USER --password-stdin ${NEXUS_REGISTRY}"
                    sh "docker push ${DOCKER_IMAGE}:${BUILD_NUMBER}"
                    sh "docker push ${DOCKER_IMAGE}:latest"
                }
            }
        }
    }

    post {
        always {
            archiveArtifacts artifacts: '*.json, *.html, artifacts/**/*', allowEmptyArchive: true
            echo 'Archiving all Industrial MLOps reports and artifacts...'
        }
        success {
            echo "Pipeline Successful! Model and Container ready for deployment."
        }
    }
}
